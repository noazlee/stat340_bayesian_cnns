{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmeGm-Q9tg8K"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "dataset_path = kagglehub.dataset_download('bhavikjikadara/dog-and-cat-classification-dataset')\n",
        "directory = os.path.join(dataset_path, 'PetImages')"
      ],
      "metadata": {
        "id": "XGeQzyvLtlG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "try:\n",
        "  for foldr in os.listdir(directory):\n",
        "    for filee in os.listdir(os.path.join(directory, foldr)):\n",
        "      images.append(os.path.join(foldr, filee))\n",
        "      labels.append(foldr)\n",
        "\n",
        "except Exception as e:\n",
        "  print(f'Error: {e}')\n",
        "\n",
        "all_df = pd.DataFrame({\n",
        "    'Images': images,\n",
        "    'Labels': labels\n",
        "    })\n",
        "\n",
        "all_df.info()"
      ],
      "metadata": {
        "id": "PdtTdbWNtlKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df.groupby(\"Labels\").count()"
      ],
      "metadata": {
        "id": "dlpll9EHtlOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i, label in enumerate(all_df['Labels'].unique()):\n",
        "    sample = all_df[all_df['Labels'] == label].sample(1).iloc[0]\n",
        "    img_path = os.path.join(directory, sample['Images'])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.subplot(1, len(all_df['Labels'].unique()), i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(label)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0tx-QbsItlRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_df, test_df = train_test_split(all_df, test_size=0.2, random_state=111620206, stratify=all_df['Labels'])"
      ],
      "metadata": {
        "id": "jA5YZlFBtlUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_small = train_df.sample(n=100, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "W7oG-_MwB3jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainimgen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.2\n",
        "    )\n",
        "\n",
        "train_data = trainimgen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=directory,\n",
        "    x_col='Images',\n",
        "    y_col='Labels',\n",
        "    target_size=(224,224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='binary',\n",
        "    batch_size=16,\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eVOlrauZu_vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testimgen = ImageDataGenerator()\n",
        "\n",
        "test_data = testimgen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    directory=directory,\n",
        "    x_col='Images',\n",
        "    y_col='Labels',\n",
        "    target_size=(224,224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='binary',\n",
        "    batch_size=16,\n",
        "    shuffle=False # For test data, it's crucial to set shuffle=False. This ensures the prediction order matches the label order, which is necessary for correct evaluation with metrics like a confusion matrix.\n",
        ")"
      ],
      "metadata": {
        "id": "D6SXljMrvH4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7siSHO0qvczu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Time"
      ],
      "metadata": {
        "id": "2n7kw_OhviqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Weight parameters (mean and rho for std via softplus)\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).normal_(-3, 0.1))\n",
        "\n",
        "        # Bias parameters\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).normal_(-3, 0.1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Sample weights: w = mu + softplus(rho) * epsilon - equivilant to Normal sample - randomness htrough epsilon, so we can get gradients from mu and sigma\n",
        "        weight_std = torch.log1p(torch.exp(self.weight_rho)) # backprop p instead of sigma - ensures sigma positive\n",
        "        weight = self.weight_mu + weight_std * torch.randn_like(self.weight_mu)\n",
        "\n",
        "        # Sample bias\n",
        "        bias_std = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias = self.bias_mu + bias_std * torch.randn_like(self.bias_mu)\n",
        "\n",
        "        # Use the sampled weights in linear pytorch function\n",
        "        return F.linear(x, weight, bias)\n",
        "\n",
        "    def kl_divergence(self):\n",
        "        # KL divergence between posterior q(w|theta) and prior p(w)\n",
        "        weight_std = torch.log1p(torch.exp(self.weight_rho))\n",
        "        bias_std = torch.log1p(torch.exp(self.bias_rho))\n",
        "\n",
        "        kl = self._kl_divergence_normal(self.weight_mu, weight_std)\n",
        "        kl += self._kl_divergence_normal(self.bias_mu, bias_std)\n",
        "        return kl\n",
        "\n",
        "    def _kl_divergence_normal(self, mu, std):\n",
        "        # KL divergence between N(mu, std) and N(0, 1)\n",
        "        kl = -torch.log(std) + (std**2 + mu**2) / 2 - 0.5\n",
        "        return kl.sum()\n",
        "\n",
        "\n",
        "class BayesianConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1):\n",
        "        super(BayesianConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "\n",
        "        # Weight parameters\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(\n",
        "            out_channels, in_channels, *self.kernel_size).normal_(0, 0.1))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(\n",
        "            out_channels, in_channels, *self.kernel_size).normal_(-3, 0.1))\n",
        "\n",
        "        # Bias parameters\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_channels).normal_(0, 0.1))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_channels).normal_(-3, 0.1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_std = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight = self.weight_mu + weight_std * torch.randn_like(self.weight_mu)\n",
        "\n",
        "        bias_std = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias = self.bias_mu + bias_std * torch.randn_like(self.bias_mu)\n",
        "\n",
        "        return F.conv2d(x, weight, bias, stride=self.stride, padding=self.padding)\n",
        "\n",
        "    def kl_divergence(self):\n",
        "        weight_std = torch.log1p(torch.exp(self.weight_rho))\n",
        "        bias_std = torch.log1p(torch.exp(self.bias_rho))\n",
        "\n",
        "        return (self._kl_divergence_normal(self.weight_mu, weight_std) +\n",
        "                self._kl_divergence_normal(self.bias_mu, bias_std))\n",
        "\n",
        "    def _kl_divergence_normal(self, mu, std):\n",
        "        kl = -torch.log(std) + (std**2 + mu**2) / 2 - 0.5\n",
        "        return kl.sum()\n",
        "\n",
        "class BayesianCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BayesianCNN, self).__init__()\n",
        "\n",
        "        # Conv block 1\n",
        "        self.conv1 = BayesianConv2d(3, 16, kernel_size=3, padding=1) # 3 channels for RGB\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv block 2\n",
        "        self.conv2 = BayesianConv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv block 3\n",
        "        self.conv3 = BayesianConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # For 224x224 input: after 3 poolings -> 28x28\n",
        "        self.fc1 = BayesianLinear(28*28*64, 128)\n",
        "        self.fc2 = BayesianLinear(128, 2)  # 2 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def kl_divergence(self):\n",
        "        # Total KL divergence for the network\n",
        "        kl = 0\n",
        "        kl += self.conv1.kl_divergence()\n",
        "        kl += self.conv2.kl_divergence()\n",
        "        kl += self.conv3.kl_divergence()\n",
        "        kl += self.fc1.kl_divergence()\n",
        "        kl += self.fc2.kl_divergence()\n",
        "        return kl\n",
        "\n",
        "    def predict_with_uncertainty(self, x, num_samples=100):\n",
        "        self.train()  # Keep in train mode to sample weights\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_samples):\n",
        "                output = self(x)\n",
        "                probs = F.softmax(output, dim=1)\n",
        "                predictions.append(probs)\n",
        "\n",
        "        predictions = torch.stack(predictions)  # [num_samples, batch_size, num_classes]\n",
        "\n",
        "        mean_predictions = predictions.mean(dim=0)\n",
        "        std_predictions = predictions.std(dim=0)\n",
        "\n",
        "        return mean_predictions, std_predictions, predictions\n",
        "\n",
        "class ParameterTracker:\n",
        "    # Track specific parameters during training\n",
        "    def __init__(self, model, num_params_to_track=10):\n",
        "        self.history = {\n",
        "            'conv1_weight_mu': [],\n",
        "            'conv1_weight_sigma': [],\n",
        "            'fc1_weight_mu': [],\n",
        "            'fc1_weight_sigma': [],\n",
        "            'fc2_bias_mu': [],\n",
        "            'fc2_bias_sigma': [],\n",
        "        }\n",
        "\n",
        "        # Randomly select indices to track\n",
        "        conv1_size = model.conv1.weight_mu.numel()\n",
        "        fc1_size = model.fc1.weight_mu.numel()\n",
        "        fc2_bias_size = model.fc2.bias_mu.numel()\n",
        "\n",
        "        self.indices = {\n",
        "            'conv1': np.random.choice(conv1_size, num_params_to_track, replace=False),\n",
        "            'fc1': np.random.choice(fc1_size, num_params_to_track, replace=False),\n",
        "            'fc2_bias': np.random.choice(fc2_bias_size, min(num_params_to_track, fc2_bias_size), replace=False),\n",
        "        }\n",
        "\n",
        "    def record(self, model):\n",
        "        # Conv1 weights\n",
        "        weight_mu = model.conv1.weight_mu.detach().cpu().flatten().numpy()\n",
        "        weight_rho = model.conv1.weight_rho.detach().cpu().flatten().numpy()\n",
        "        weight_sigma = np.log1p(np.exp(weight_rho))\n",
        "        self.history['conv1_weight_mu'].append(weight_mu[self.indices['conv1']])\n",
        "        self.history['conv1_weight_sigma'].append(weight_sigma[self.indices['conv1']])\n",
        "\n",
        "        # FC1 weights\n",
        "        weight_mu = model.fc1.weight_mu.detach().cpu().flatten().numpy()\n",
        "        weight_rho = model.fc1.weight_rho.detach().cpu().flatten().numpy()\n",
        "        weight_sigma = np.log1p(np.exp(weight_rho))\n",
        "        self.history['fc1_weight_mu'].append(weight_mu[self.indices['fc1']])\n",
        "        self.history['fc1_weight_sigma'].append(weight_sigma[self.indices['fc1']])\n",
        "\n",
        "        # FC2 biases\n",
        "        bias_mu = model.fc2.bias_mu.detach().cpu().numpy()\n",
        "        bias_rho = model.fc2.bias_rho.detach().cpu().numpy()\n",
        "        bias_sigma = np.log1p(np.exp(bias_rho))\n",
        "        self.history['fc2_bias_mu'].append(bias_mu[self.indices['fc2_bias']])\n",
        "        self.history['fc2_bias_sigma'].append(bias_sigma[self.indices['fc2_bias']])\n",
        "\n",
        "    def plot_evolution(self):\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Parameter Evolution During Training', fontsize=16)\n",
        "\n",
        "        epochs = np.arange(len(self.history['conv1_weight_mu']))\n",
        "\n",
        "        # Conv1 weight mu\n",
        "        for i in range(len(self.indices['conv1'])):\n",
        "            mu_values = [epoch_vals[i] for epoch_vals in self.history['conv1_weight_mu']]\n",
        "            axes[0, 0].plot(epochs, mu_values, alpha=0.7, label=f'Weight {i}')\n",
        "        axes[0, 0].set_title('Conv1 Weight μ Evolution')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('μ value')\n",
        "        axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "        # Conv1 weight sigma\n",
        "        for i in range(len(self.indices['conv1'])):\n",
        "            sigma_values = [epoch_vals[i] for epoch_vals in self.history['conv1_weight_sigma']]\n",
        "            axes[0, 1].plot(epochs, sigma_values, alpha=0.7, label=f'Weight {i}')\n",
        "        axes[0, 1].set_title('Conv1 Weight σ Evolution')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('σ value')\n",
        "        axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "        # FC1 weight mu\n",
        "        for i in range(len(self.indices['fc1'])):\n",
        "            mu_values = [epoch_vals[i] for epoch_vals in self.history['fc1_weight_mu']]\n",
        "            axes[1, 0].plot(epochs, mu_values, alpha=0.7)\n",
        "        axes[1, 0].set_title('FC1 Weight μ Evolution')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('μ value')\n",
        "        axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "        # FC1 weight sigma\n",
        "        for i in range(len(self.indices['fc1'])):\n",
        "            sigma_values = [epoch_vals[i] for epoch_vals in self.history['fc1_weight_sigma']]\n",
        "            axes[1, 1].plot(epochs, sigma_values, alpha=0.7)\n",
        "        axes[1, 1].set_title('FC1 Weight σ Evolution')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('σ value')\n",
        "        axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "        # FC2 bias mu\n",
        "        for i in range(len(self.indices['fc2_bias'])):\n",
        "            mu_values = [epoch_vals[i] for epoch_vals in self.history['fc2_bias_mu']]\n",
        "            axes[2, 0].plot(epochs, mu_values, alpha=0.7, marker='o', markersize=3)\n",
        "        axes[2, 0].set_title('FC2 Bias μ Evolution')\n",
        "        axes[2, 0].set_xlabel('Epoch')\n",
        "        axes[2, 0].set_ylabel('μ value')\n",
        "        axes[2, 0].grid(alpha=0.3)\n",
        "\n",
        "        # FC2 bias sigma\n",
        "        for i in range(len(self.indices['fc2_bias'])):\n",
        "            sigma_values = [epoch_vals[i] for epoch_vals in self.history['fc2_bias_sigma']]\n",
        "            axes[2, 1].plot(epochs, sigma_values, alpha=0.7, marker='o', markersize=3)\n",
        "        axes[2, 1].set_title('FC2 Bias σ Evolution')\n",
        "        axes[2, 1].set_xlabel('Epoch')\n",
        "        axes[2, 1].set_ylabel('σ value')\n",
        "        axes[2, 1].axhline(1, color='red', linestyle='--', alpha=0.5, label='Prior σ=1')\n",
        "        axes[2, 1].grid(alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "_DaIm-MpviMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = BayesianCNN().to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "ZZa7tDOmvosN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_data will have {len(train_data)} batches\")"
      ],
      "metadata": {
        "id": "XsX6HmqcDeSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/STATS/'"
      ],
      "metadata": {
        "id": "dqZD0-wsBHco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "tracker = ParameterTracker(model, num_params_to_track=10)\n",
        "tracker.record(model)\n",
        "\n",
        "# Calculate beta based on total number of training samples\n",
        "beta = 1.0 / len(train_df)\n",
        "print(f\"Beta: {beta:.8f}\")\n",
        "\n",
        "num_epochs = 30\n",
        "num_batches = len(train_data)\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_accuracy = 0.0\n",
        "patience = 5  # Number of epochs to wait for improvement\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_nll = 0\n",
        "    total_kl = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_data):\n",
        "        if batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        data = torch.FloatTensor(data).to(device)\n",
        "        target = torch.LongTensor(target.astype(int)).to(device)\n",
        "\n",
        "        # Normalize pixel values to [0, 1] if not already done\n",
        "        if data.max() > 1.0:\n",
        "            data = data / 255.0\n",
        "\n",
        "        # Transpose from (batch, height, width, channels) to (batch, channels, height, width)\n",
        "        data = data.permute(0, 3, 1, 2)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        nll_loss = F.cross_entropy(output, target)\n",
        "        kl_loss = model.kl_divergence()\n",
        "        loss = nll_loss + beta * kl_loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_nll += nll_loss.item()\n",
        "        total_kl += kl_loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_data)\n",
        "    avg_nll = total_nll / len(train_data)\n",
        "    avg_kl = total_kl / len(train_data)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    tracker.record(model)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'  Loss: {avg_loss:.4f} (NLL: {avg_nll:.4f}, KL: {avg_kl:.4f})')\n",
        "\n",
        "    # ============ VALIDATION ACCURACY ============\n",
        "    # Calculate validation accuracy every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(test_data):\n",
        "                if batch_idx >= len(test_data):\n",
        "                    break\n",
        "\n",
        "                # Convert to tensors\n",
        "                data = torch.FloatTensor(data).to(device)\n",
        "                target = torch.LongTensor(target.astype(int)).to(device)\n",
        "\n",
        "                # Normalize\n",
        "                if data.max() > 1.0:\n",
        "                    data = data / 255.0\n",
        "\n",
        "                # Transpose\n",
        "                data = data.permute(0, 3, 1, 2)\n",
        "\n",
        "                # Forward pass (single sample for speed)\n",
        "                outputs = model(data)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        print(f'  Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_losses': train_losses,\n",
        "                'val_accuracies': val_accuracies,\n",
        "                'best_val_accuracy': best_val_accuracy,\n",
        "            }, f'{save_dir}/best_bayesian_cnn_catdog.pth')\n",
        "            print(f'  ✓ New best model saved! (Val Acc: {val_accuracy:.2f}%)')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f'  No improvement. Patience: {patience_counter}/{patience}')\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f'\\nEarly stopping triggered at epoch {epoch+1}')\n",
        "                print(f'Best validation accuracy: {best_val_accuracy:.2f}%')\n",
        "                break\n",
        "\n",
        "    # Save periodic checkpoints every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_losses': train_losses,\n",
        "            'val_accuracies': val_accuracies,\n",
        "        }, f'{save_dir}/checkpoint_epoch_{epoch+1}.pth')\n",
        "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "torch.save({\n",
        "    'epoch': epoch + 1,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'val_accuracies': val_accuracies,\n",
        "    'tracker_history': tracker.history,\n",
        "    'best_val_accuracy': best_val_accuracy,\n",
        "}, f'{save_dir}/bayesian_cnn_catdog_final.pth')\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final model saved as 'bayesian_cnn_catdog_final.pth'\")\n",
        "print(f\"Best model saved as 'best_bayesian_cnn_catdog.pth'\")\n",
        "print(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "AyMLBfmnwEU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss and validation accuracy\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot training loss\n",
        "ax1.plot(range(1, len(train_losses) + 1), train_losses, marker='o')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Training Loss')\n",
        "ax1.set_title('Training Loss over Epochs (Bayesian CNN)')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot validation accuracy\n",
        "val_epochs = list(range(2, 2 * len(val_accuracies) + 1, 2))\n",
        "ax2.plot(val_epochs, val_accuracies, marker='o', color='green')\n",
        "ax2.axhline(y=best_val_accuracy, color='r', linestyle='--', label=f'Best: {best_val_accuracy:.2f}%')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Validation Accuracy (%)')\n",
        "ax2.set_title('Validation Accuracy over Epochs (Bayesian CNN)')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Training Loss: {train_losses[-1]:.4f}\")\n",
        "if len(val_accuracies) > 0:\n",
        "    print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")"
      ],
      "metadata": {
        "id": "qeThXnWRTyFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BayesianCNN().to(device)\n",
        "model.load_state_dict(torch.load(f'{save_dir}/checkpoint_epoch_10.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "QQ7TueXrv0Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aRCzsD8ahYby"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}